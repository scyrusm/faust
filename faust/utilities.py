import pandas as pd
import numpy as np
import os


def read_gpp_output(
        folders,
        chipfile=None,
        chipfile_gene_symbol_colname='Gene Symbol',
        barcode2gene_dict=None,
        indices=['Construct Barcode', 'Construct IDs', 'UMI', 'Target Gene'],
        dropcols=[],
        umi_col='UMI',
        collapse_umis=False):
    """Helper function designed to read output generated at the Broad Gene Perturbation Platform (GPP).
    Output takes the form of multiple folders with counts matrices generated via PoolQ3. The chipfile is used to
    create a mapping between a barcode sequence and a gene (using the columns "Barcode Sequence" and "Gene Symbol" within
    the chip file). If no chip file is present, one may simply pass a dictionary with this mapping as the optional argument
    "barcode2gene_dict". This function will then loop through all the files in the specified folders (ignoring those with "MATCH" in the
    filename, indicating counts that could not be matched to a specific UMI), make a note of their UMI, and concatenate them.
    These filenames should have the form "<something>-XXXXXX.txt", where "XXXXXX" is the umi.

    Parameters
    ----------
    folders : str, path to folders containing .txt files generated by the GPP
        
    chipfile : str, path to chip file (usually with the suffix '.chip')
        (Default value = None)
    barcode2gene_dict : dict, mapping between "Barcode Sequence" and "Gene Symbol"
        (Default value = None)
    indices : str, column in .txt files to use as the index (relevant when summing the counts from different plates together)
        (Default value = ['Construct Barcode', 'Construct IDs', 'UMI' , 'Target Gene'] :
    dropcols : list, list of columns in txt file to drop
        (Default value = [])
    'Construct IDs' :
        
    'UMI' :
        
    'Target Gene'] :
        

    Returns
    -------

    
    """
    if chipfile is None and barcode2gene_dict is None:
        raise Exception(
            "One of barcode2gene_dict or chipfile must not be None")
    if barcode2gene_dict is None:
        chip = pd.read_table(chipfile)
        if chipfile_gene_symbol_colname not in chip.columns:
            raise Exception(
                "key {} not one of {}, the columns in chipfile".format(
                    chipfile_gene_symbol_colname, chip.columns))
        barcode2gene_dict = chip.set_index(
            'Barcode Sequence')[chipfile_gene_symbol_colname].to_dict()
    if type(folders) is str:
        folders = [folders]
    dfss = []
    for folder in folders:
        dfs = []
        for txt in [
                x for x in os.listdir(folder)
                if 'MATCH' not in x and not x.startswith('.')
        ]:
            df = pd.read_table(folder + '/' + txt)
            umi = txt.split('-')[-1].replace('.txt', '')
            df[umi_col] = umi
            dfs.append(df)
        dfs = pd.concat(dfs)
        dfs['Target Gene'] = [
            barcode2gene_dict[x] for x in dfs['Construct Barcode']
        ]
        dfss.append(dfs.copy())
    dfss = [
        x[[y for y in x.columns if y not in dropcols]].set_index(indices)
        for x in dfss
    ]
    gpp_df = sum(dfss).reset_index()
    if collapse_umis:
         gpp_df = gpp_df.drop(umi_col,axis=1).groupby([x for x in indices if x!=umi_col]).sum().reset_index()
    return gpp_df


def get_summary_df(df,
                   controls,
                   inputs,
                   outputs,
                   input_type='single',
                   verbose=True,
                   count_threshold=0,
                   estimate_cells=True,
                   gene_col='Target Gene',
                   alternative='two-sided',
                   downsample_control=False,
                   custom_test=None,
                   custom_effect_size=None,
                   progress_logger=None
                   ):
    """

    Parameters
    ----------
    df :
        
    controls :
        
    inputs :
        
    outputs :
        
    input_type :
        (Default value = 'single')
    verbose :
        (Default value = True)
    count_threshold :
        (Default value = 1)
    estimate_cells :
        (Default value = True)
    gene_col :
        (Default value = 'Target Gene')
    alternative :
        (Default value = 'two-sided')
    downsample_control :
        (Default value = False)
    custom_test :
         (Default value = None)
    custom_effect_size :
         (Default value = None)

    Returns
    -------

    
    """
    from tqdm import tqdm
    import pandas as pd
    from faust.utilities import nan_fdrcorrection_q
    from scipy.stats import mannwhitneyu

    if input_type not in ['single', 'matched']:
        raise Exception("input_type must be one of 'single', 'matched'")
    df = df.copy()
    if input_type == 'single':
        df['Input Sum'] = df[inputs].sum(axis=1)
        df = df[df['Input Sum'] > count_threshold]
        inputs = ['Input Sum'] * len(outputs)
    else:
        if len(outputs) != len(inputs):
            raise Exception(
                "outputs and inputs must be equal length when input_type is 'matched'"
            )
    output2input_dict = {}
    for output_col, input_col in zip(outputs, inputs):
        df[output_col] = np.where(df[output_col] > count_threshold,
                                  df[output_col], 0)
        df[input_col] = np.where(df[input_col] > count_threshold,
                                 df[input_col], 0)
        if verbose:
            print("Comparing input", input_col, "with output", output_col)
        df[output_col + '_odds_ratio'] = np.divide(df[output_col],
                                                   df[input_col])
        output2input_dict[output_col] = input_col

    constructs_ids = []
    cless = []
    output_sites = []
    input_sites = []
    mws = []
    custom_tests = []
    custom_effect_sizes = []
    if estimate_cells:
        from faust.utilities import estimate_cell_input
        estimated_cells_input = []
        estimated_cells_output = []
    construct_ids = df[gene_col].unique()
    if progress_logger is not None:
        progress=0
        progress_logger.setValue(progress)
    if verbose:
        construct_ids = tqdm(construct_ids,
                             desc='Looping through target genes')
    for construct_id in construct_ids:
        experiment = df[df[gene_col] == construct_id]
        control = df[df[gene_col].isin(controls) & (
            df[gene_col] != construct_id
        )]  # This is so there is no overlap between the experiment and control groups
        for odds_ratio in [
                x for x in experiment.columns if x.endswith('_odds_ratio')
        ]:
            a = experiment[odds_ratio].values
            b = control[odds_ratio].values
            if downsample_control is not False:
                if type(downsample_control) is not float:
                    if (downsamplecontrol <= 0) or (downsample_control >= 1):
                        raise Exception(
                            "downsample_control must be False, or a float in (0,1)"
                        )
                subsample_mask = np.random.choice(
                    [False, True],
                    replace=True,
                    p=[1 - downsample_control, downsample_control],
                    size=len(b))
                b = b[subsample_mask]

            if input_type == 'matched':
                a = [x for x in a if not np.isinf(x) and not np.isnan(x)]
                b = [x for x in b if not np.isinf(x) and not np.isnan(x)]
            if len(a) == 0 or len(b) == 0:
                cless.append(np.nan)
                mws.append(np.nan)
                if custom_test is not None:
                    custom_tests.append(np.nan)
                if custom_effect_size is not None:
                    custom_effect_sizes.append(np.nan)

            else:
                mw = mannwhitneyu(a, b, alternative=alternative)
                cless.append(mw.statistic / len(a) / len(b))
                mws.append(mw.pvalue)
                if custom_test is not None:
                    custom_tests.append(custom_test(a, b))
                if custom_effect_size is not None:
                    custom_effect_sizes.append(custom_effect_size(a, b))
            output_site = odds_ratio.split('_odds_ratio')[0]
            output_sites.append(output_site)
            input_site = output2input_dict[odds_ratio.split('_odds_ratio')[0]]
            input_sites.append(input_site)
            constructs_ids.append(construct_id)
            if estimate_cells:
                estimated_cells_input.append(
                    estimate_cell_input(df,
                                        input_site,
                                        construct_id,
                                        count_threshold,
                                        target_gene_col=gene_col))
                estimated_cells_output.append(
                    estimate_cell_input(df,
                                        output_site,
                                        construct_id,
                                        count_threshold,
                                        target_gene_col=gene_col))
        if progress_logger is not None:
            progress+=100/len(construct_ids)
            progress_logger.setValue(progress)

    summary_df = pd.DataFrame(constructs_ids, columns=['gene'])
    summary_df['output_site'] = output_sites
    summary_df['input_site'] = input_sites
    summary_df['CommonLanguageEffectSize'] = cless
    summary_df['MannWhitneyP'] = mws
    summary_df['BH_q'] = nan_fdrcorrection_q(summary_df['MannWhitneyP'].values)
    if custom_test is not None:
        summary_df['CustomTest'] = custom_tests
        summary_df['CustomTest_BH_q'] = nan_fdrcorrection_q(
            summary_df['CustomTest'].values)
    if custom_effect_size is not None:
        summary_df['CustomEffectSize'] = custom_effect_sizes
    if estimate_cells:
        summary_df['input_estimated_cell_count'] = estimated_cells_input
        summary_df['output_estimated_cell_count'] = estimated_cells_output
    return summary_df


def get_summary_df_v2(df,
                      controls,
                      inputs,
                      outputs,
                      input_type='single',
                      verbose=True,
                      count_threshold=1,
                      estimate_cells=True,
                      gene_col='Target Gene',
                      alternative='two-sided',
                      downsample_control=False,
                      custom_test=None,
                      custom_effect_size=None,
                      assume_equal_inputs=False,
                      run_parallel=True,
                      control_and_experiment_exclusive=True):
    """

    Parameters
    ----------
    df :
        
    controls :
        
    inputs :
        
    outputs :
        
    input_type :
        (Default value = 'single')
    verbose :
        (Default value = True)
    count_threshold :
        (Default value = 1)
    estimate_cells :
        (Default value = True)
    gene_col :
        (Default value = 'Target Gene')
    alternative :
        (Default value = 'two-sided')
    downsample_control :
        (Default value = False)
    custom_test :
         (Default value = None)
    custom_effect_size :
         (Default value = None)
    assume_equal_inputs :
         (Default value = False)
    run_parallel :
         (Default value = True)
    control_and_experiment_exclusive :
         (Default value = True)

    Returns
    -------

    
    """
    from tqdm import tqdm
    import pandas as pd
    from faust.utilities import nan_fdrcorrection_q
    from scipy.stats import mannwhitneyu
    if input_type == 'single':
        if run_parallel:
            from joblib import Parallel, delayed
            return pd.concat(
                Parallel(n_jobs=-1, )(delayed(get_summary_df_v2)
                                      (df[[gene_col, col] + inputs],
                                       df[gene_col].unique(),
                                       inputs, [col],
                                       run_parallel=False,
                                       input_type=input_type,
                                       verbose=verbose,
                                       count_threshold=count_threshold,
                                       estimate_cells=estimate_cells,
                                       gene_col=gene_col,
                                       alternative=alternative,
                                       downsample_control=downsample_control,
                                       custom_test=custom_test,
                                       custom_effect_size=custom_effect_size,
                                       assume_equal_inputs=assume_equal_inputs,
                                       control_and_experiment_exclusive=
                                       control_and_experiment_exclusive)
                                      for col in outputs))
        elif len(outputs) > 1:
            return pd.concat([
                get_summary_df_v2(df[[gene_col, col] + inputs],
                                  df[gene_col].unique(),
                                  inputs, [col],
                                  run_parallel=False,
                                  input_type=input_type,
                                  verbose=verbose,
                                  count_threshold=count_threshold,
                                  estimate_cells=estimate_cells,
                                  gene_col=gene_col,
                                  alternative=alternative,
                                  downsample_control=downsample_control,
                                  custom_test=custom_test,
                                  custom_effect_size=custom_effect_size,
                                  assume_equal_inputs=assume_equal_inputs,
                                  control_and_experiment_exclusive=
                                  control_and_experiment_exclusive)
                for col in outputs
            ])
    elif input_type == 'matched':
        if run_parallel:
            from joblib import Parallel, delayed
            return pd.concat(
                Parallel(n_jobs=4, )(delayed(get_summary_df_v2)
                                     (df[[gene_col, col] + inputs],
                                      df[gene_col].unique(), [input], [col],
                                      run_parallel=False,
                                      input_type=input_type,
                                      verbose=verbose,
                                      count_threshold=count_threshold,
                                      estimate_cells=estimate_cells,
                                      gene_col=gene_col,
                                      alternative=alternative,
                                      downsample_control=downsample_control,
                                      custom_test=custom_test,
                                      custom_effect_size=custom_effect_size,
                                      assume_equal_inputs=assume_equal_inputs,
                                      control_and_experiment_exclusive=
                                      control_and_experiment_exclusive)
                                     for input, col in zip(inputs, outputs)))
        elif len(outputs) > 1:
            return pd.concat([
                get_summary_df_v2(df[[gene_col, col] + inputs],
                                  df[gene_col].unique(), [input], [col],
                                  run_parallel=False,
                                  input_type=input_type,
                                  verbose=verbose,
                                  count_threshold=count_threshold,
                                  estimate_cells=estimate_cells,
                                  gene_col=gene_col,
                                  alternative=alternative,
                                  downsample_control=downsample_control,
                                  custom_test=custom_test,
                                  custom_effect_size=custom_effect_size,
                                  assume_equal_inputs=assume_equal_inputs,
                                  control_and_experiment_exclusive=
                                  control_and_experiment_exclusive)
                for input, col in zip(inputs, outputs)
            ])

    if input_type not in ['single', 'matched']:
        raise Exception("input_type must be one of 'single', 'matched'")
    df = df.copy()
    if assume_equal_inputs:
        df['Unit Input'] = count_threshold + 1
        inputs = ['Unit Input'] * len(outputs)
        print('Assuming equal inputs (ignoring "inputs" argument)')
    elif input_type == 'single':
        df['Input Sum'] = df[inputs].sum(axis=1)
        df = df[df['Input Sum'] > count_threshold]
        inputs = ['Input Sum'] * len(outputs)
    else:
        if len(outputs) != len(inputs):
            raise Exception(
                "outputs and inputs must be equal length when input_type is 'matched'"
            )
    output2input_dict = {}
    for output_col, input_col in zip(outputs, inputs):
        df[output_col] = np.where(df[output_col] > count_threshold,
                                  df[output_col], 0)
        df[input_col] = np.where(df[input_col] > count_threshold,
                                 df[input_col], 0)
        if verbose:
            print("Comparing input", input_col, "with output", output_col)
        df[output_col + '_odds_ratio'] = np.divide(df[output_col],
                                                   df[input_col])
        output2input_dict[output_col] = input_col

    constructs_ids = []
    cless = []
    output_sites = []
    input_sites = []
    mws = []
    custom_tests = []
    custom_effect_sizes = []
    if estimate_cells:
        from faust.utilities import estimate_cell_input
        estimated_cells_input = []
        estimated_cells_output = []
    construct_ids = df[gene_col].unique()
    if verbose:
        construct_ids = tqdm(
            construct_ids,
            desc='Looping through target genes, output: {}'.format(outputs))
    if not control_and_experiment_exclusive:
        control = df.query('`{0}` in @controls'.format(gene_col))
        if downsample_control is not False:
            if type(downsample_control) is not float:
                if (downsamplecontrol <= 0) or (downsample_control >= 1):
                    raise Exception(
                        "downsample_control must be False, or a float in (0,1)"
                    )
            subsample_mask = np.random.choice(
                [False, True],
                replace=True,
                p=[1 - downsample_control, downsample_control],
                size=len(control))
            control = control[subsample_mask]
    for construct_id in construct_ids:
        experiment = df.query('`{}`==@construct_id'.format(gene_col))
        if control_and_experiment_exclusive:
            control = df.query('`{0}` in @controls'.format(gene_col)).query(
                '`{}`!=@construct_id'.format(gene_col))
            if downsample_control is not False:
                if type(downsample_control) is not float:
                    if (downsamplecontrol <= 0) or (downsample_control >= 1):
                        raise Exception(
                            "downsample_control must be False, or a float in (0,1)"
                        )
                subsample_mask = np.random.choice(
                    [False, True],
                    replace=True,
                    p=[1 - downsample_control, downsample_control],
                    size=len(control))
                control = control[subsample_mask]
        for odds_ratio in [
                x for x in experiment.columns if x.endswith('_odds_ratio')
        ]:
            a = experiment[odds_ratio].values
            b = control[odds_ratio].values

            if input_type == 'matched':
                a = [x for x in a if not np.isinf(x) and not np.isnan(x)]
                b = [x for x in b if not np.isinf(x) and not np.isnan(x)]
            if len(a) == 0 or len(b) == 0:
                cless.append(np.nan)
                mws.append(np.nan)
                if custom_test is not None:
                    custom_tests.append(np.nan)
                if custom_effect_size is not None:
                    custom_effect_sizes.append(np.nan)

            else:
                mw = mannwhitneyu(a, b, alternative=alternative)
                cless.append(mw.statistic / len(a) / len(b))
                mws.append(mw.pvalue)
                if custom_test is not None:
                    custom_tests.append(custom_test(a, b))
                if custom_effect_size is not None:
                    custom_effect_sizes.append(custom_effect_size(a, b))
            output_site = odds_ratio.split('_odds_ratio')[0]
            output_sites.append(output_site)
            input_site = output2input_dict[odds_ratio.split('_odds_ratio')[0]]
            input_sites.append(input_site)
            constructs_ids.append(construct_id)
            if estimate_cells:
                estimated_cells_output.append(
                    estimate_cell_input(df,
                                        output_site,
                                        construct_id,
                                        count_threshold,
                                        target_gene_col=gene_col))

    summary_df = pd.DataFrame(constructs_ids, columns=['gene'])
    summary_df['output_site'] = output_sites
    summary_df['input_site'] = input_sites
    summary_df['CommonLanguageEffectSize'] = cless
    summary_df['MannWhitneyP'] = mws
    summary_df['BH_q'] = nan_fdrcorrection_q(summary_df['MannWhitneyP'].values)
    if custom_test is not None:
        summary_df['CustomTest'] = custom_tests
    if custom_effect_size is not None:
        summary_df['CustomEffectSize'] = custom_effect_sizes
    if estimate_cells:
        summary_df['output_estimated_cell_count'] = estimated_cells_output
    return summary_df


def nan_fdrcorrection_q(pvalues):
    """

    Parameters
    ----------
    pvalues :
        

    Returns
    -------

    
    """
    from statsmodels.stats.multitest import fdrcorrection
    pvalues = np.array(pvalues)
    realmask = ~np.isnan(pvalues)
    qvalues = np.array([np.nan] * len(pvalues))
    qvalues[realmask] = fdrcorrection(pvalues[realmask])[1]
    return qvalues


def get_replicate_aggregated_statistics(summary_df,
                                        aggregation_column=None,
                                        inplace=False,
                                        pvalue_col_name='MannWhitneyP'):
    """

    Parameters
    ----------
    summary_df :
        
    aggregation_column :
        (Default value = None)
    inplace :
        (Default value = False)
    pvalue_col_name :
         (Default value = 'MannWhitneyP')

    Returns
    -------

    
    """
    if aggregation_column not in summary_df.columns:
        raise Exception("aggregation_column must be column in summary_df")

    from statsmodels.stats.multitest import fdrcorrection
    from scipy.stats import combine_pvalues
    from faust.utilities import nan_fdrcorrection_q

    if not inplace:
        summary_df = summary_df.copy()
    for aggregation in summary_df[aggregation_column].unique():

        aggregate_gene_max_p = summary_df[
            summary_df[aggregation_column] == aggregation].groupby(
                'gene')[pvalue_col_name].apply(max)
        gene2aggregate_union_q = {
            gene: q
            for gene, q in zip(
                aggregate_gene_max_p.index.values,
                nan_fdrcorrection_q(aggregate_gene_max_p.values))
        }
        summary_df['{}_union_BH_q'.format(aggregation)] = summary_df[
            'gene'].apply(lambda x: gene2aggregate_union_q[x])

        aggregate_gene_fisher_combined_p = summary_df[
            summary_df[aggregation_column] == aggregation].groupby('gene')[
                pvalue_col_name].apply(combine_pvalues).apply(lambda x: x[1])
        gene2aggregate_intersection_q = {
            gene: q
            for gene, q in zip(
                aggregate_gene_fisher_combined_p.index.values,
                nan_fdrcorrection_q(aggregate_gene_fisher_combined_p.values))
        }
        summary_df['{}_intersection_BH_q'.format(aggregation)] = summary_df[
            'gene'].apply(lambda x: gene2aggregate_intersection_q[x])
    if not inplace:
        return summary_df


def estimate_read_error_singlets(n_observed_unique_grna_umis, n_observed_zeros,
                                 log2_counts_sum):
    """

    Parameters
    ----------
    n_observed_unique_grna_umis :
        
    n_observed_zeros :
        
    log2_counts_sum :
        

    Returns
    -------

    
    """
    from scipy.optimize import bisect

    left_side = lambda false_zeros: np.log(n_observed_unique_grna_umis /
                                           (n_observed_zeros - false_zeros))
    right_side = lambda false_zeros: log2_counts_sum / (
        n_observed_unique_grna_umis - false_zeros)
    f = lambda false_zeros: left_side(false_zeros) - right_side(false_zeros)
    estimate = bisect(f, 0, n_observed_zeros)
    return estimate / n_observed_zeros


def predicted_input(n_possible_grna_umi, n_detected_grna_umi):
    """

    Parameters
    ----------
    n_possible_grna_umi :
        
    n_detected_grna_umi :
        

    Returns
    -------

    
    """
    return n_possible_grna_umi * np.log(
        n_possible_grna_umi / (n_possible_grna_umi - n_detected_grna_umi))


def estimate_cell_input(df,
                        sample,
                        target_gene,
                        count_threshold,
                        target_gene_col='Target Gene'):
    """

    Parameters
    ----------
    df :
        
    sample :
        
    target_gene :
        
    count_threshold :
        
    target_gene_col :
        (Default value = 'Target Gene')

    Returns
    -------

    
    """
    if type(target_gene) is str:
        target_gene = [target_gene]
    from faust.utilities import predicted_input
    n_detected_grna_umi = (df[(df[target_gene_col].isin(target_gene)) & \
                             (df[sample]>count_threshold)][sample]>0).sum()
    n_possible_grna_umi = df[df[target_gene_col].isin(target_gene)].shape[0]

    if n_possible_grna_umi == n_detected_grna_umi:
        #        from IPython.core.debugger import set_trace; set_trace()
        print(n_possible_grna_umi, n_detected_grna_umi)
        print(
            "Warning:  n_possible_grna_umi == n_detected_grna_umi. Poisson approximation invalid."
        )
        return np.nan
    else:
        return predicted_input(n_possible_grna_umi, n_detected_grna_umi)


def get_mageck_compatible_df(df,
                             sgRNA_col='Construct Barcode',
                             gene_col='Construct IDs',
                             UMI_col='UMI',
                             append_UMI=True,
                             output=None):
    """

    Parameters
    ----------
    df :
        
    sgRNA_col :
        (Default value = 'Construct Barcode')
    gene_col :
        (Default value = 'Construct IDs')
    UMI_col :
        (Default value = 'UMI')
    append_UMI :
        (Default value = True)
    output :
        (Default value = None)

    Returns
    -------

    
    """
    df = df.copy()
    if append_UMI:
        df['sgRNA'] = df[sgRNA_col] + '_' + df[UMI_col]
    else:
        df['sgRNA'] = df[sgRNA_col]
    df['gene'] = df[gene_col]
    df = df[['sgRNA', 'gene'] +
            list(df.columns[(df.dtypes == int) | (df.dtypes == float)])]
    if output is not None:
        if output.endswith('.csv'):
            df.to_csv(output, index=False)
        else:
            df.to_csv(output, index=False, sep='\t')
    return df


def get_mageck_ibar_compatible_df(df,
                                  sgRNA_col='Construct Barcode',
                                  gene_col='Construct IDs',
                                  UMI_col='UMI',
                                  output=None):
    """

    Parameters
    ----------
    df :
        
    sgRNA_col :
        (Default value = 'Construct Barcode')
    gene_col :
        (Default value = 'Construct IDs')
    UMI_col :
        (Default value = 'UMI')
    output :
        (Default value = None)

    Returns
    -------

    
    """
    df = df.copy()

    df['guide'] = df[sgRNA_col]
    df['gene'] = df[gene_col]
    df['barcode'] = df[UMI_col]
    df = df[['gene', 'guide', 'barcode'] + list(df.columns[df.dtypes == int])]
    if output is not None:
        if not output.endswith('.csv'):
            output = output + '.csv'
        df.to_csv(output, index=False)
    return df


def get_zfc_compatible_df(df,
                          sgRNA_col='Construct Barcode',
                          gene_col='Construct IDs',
                          UMI_col='UMI',
                          ctrl_col=None,
                          exp_col=None,
                          output=None):
    """

    Parameters
    ----------
    df :
        
    sgRNA_col :
        (Default value = 'Construct Barcode')
    gene_col :
        (Default value = 'Construct IDs')
    UMI_col :
        (Default value = 'UMI')
    ctrl_col :
        (Default value = None)
    exp_col :
        (Default value = None)
    output :
        (Default value = None)

    Returns
    -------

    
    """
    df = df.copy()

    df['guide'] = df[sgRNA_col]
    df['gene'] = df[gene_col]
    df['barcode'] = df[UMI_col]
    df['ctrl'] = df[ctrl_col]
    df['exp'] = df[exp_col]
    df = df[['gene', 'guide', 'barcode', 'ctrl', 'exp']]
    if output is not None:
        df.to_csv(output, index=False, sep='\t')
    return df


def get_riger_compatible_df(df,
                            sgRNA_col='Construct Barcode',
                            UMI_col='UMI',
                            append_UMI=True,
                            gene_col='Construct IDs',
                            score_col=None,
                            rank_col=None):
    """

    Parameters
    ----------
    df :
        
    sgRNA_col :
        (Default value = 'Construct Barcode')
    UMI_col :
        (Default value = 'UMI')
    append_UMI :
        (Default value = True)
    gene_col :
        (Default value = 'Construct IDs')
    score_col :
        (Default value = None)
    rank_col :
        (Default value = None)

    Returns
    -------

    
    """
    df = df.copy()
    if append_UMI:
        df['Construct'] = df[sgRNA_col] + '_' + df[UMI_col]
    else:
        df['Construct'] = df[sgRNA_col]
    df['GeneSymbol'] = df[gene_col]
    df['NormalizedScore'] = df[score_col]
    df = df.sort_values(rank_col)
    df['Construct Rank'] = df.reset_index().index.values
    df['HairpinWeight'] = 1.0
    # Construct
    #    The name of the hairpin.
    # GeneSymbol
    #    A unique name for the gene.
    # NormalizedScore
    #   The hairpin score.
    # Construct Rank
    #    The hairpin rank.
    # HairpinWeight
    #  0 to 1--0 is unweighted
    return df[[
        'Construct', 'GeneSymbol', 'NormalizedScore', 'Construct Rank',
        'HairpinWeight'
    ]]


def run_alternative_test(df,
                         test=None,
                         exp_col=None,
                         ctrl_col=None,
                         sgRNA_col='Construct Barcode',
                         gene_col='Construct IDs',
                         UMI_col='UMI',
                         output=''):
    """

    Parameters
    ----------
    df :
        
    test :
        (Default value = None)
    exp_col :
        (Default value = None)
    ctrl_col :
        (Default value = None)
    sgRNA_col :
        (Default value = 'Construct Barcode')
    gene_col :
        (Default value = 'Construct IDs')
    UMI_col :
        (Default value = 'UMI')
    output :
        (Default value = '')

    Returns
    -------

    
    """
    from faust.utilities import get_mageck_compatible_df, get_mageck_ibar_compatible_df, get_zfc_compatible_df
    implemented_tests = ['mageck', 'mageck-ibar', 'zfc']
    if test not in implemented_tests:  #+['riger']:
        raise Exception("test must be one of {}".format(implemented_tests))
    if output == '' or type(output) != str:
        raise Exception("output must be a non-null string")
    if type(exp_col) is str:
        if test == 'mageck':
            transformed_df = get_mageck_compatible_df(df,
                                                      output=output,
                                                      gene_col=gene_col,
                                                      sgRNA_col=sgRNA_col,
                                                      UMI_col=UMI_col)
            maxreps = transformed_df['gene'].value_counts().max() + 1
            command = 'mageck test -k {0} -t "{1}" -c "{2}" -n {0} --additional-rra-parameters " --max-sgrnapergene-permutation {3}"'.format(
                output, exp_col, ctrl_col, maxreps)
            primary_output = "{}.gene_summary.txt".format(output)
            secondary_output = None
        elif test == 'mageck-ibar':
            transformed_df = get_mageck_ibar_compatible_df(df,
                                                           output=output,
                                                           gene_col=gene_col,
                                                           sgRNA_col=sgRNA_col,
                                                           UMI_col=UMI_col)
            maxreps = transformed_df['gene'].value_counts().max() + 1
            if not output.endswith('.csv'):
                output = output + '.csv'
            command = 'mageck-ibar --RRApath "RRA --max-sgrnapergene-permutation {3}" -i {0} -t "{1}" -c "{2}" -o {0}'.format(
                output, exp_col, ctrl_col, maxreps)
            primary_output = output + ".gene.high.txt"
            secondary_output = output + ".gene.low.txt"
        elif test == 'zfc':
            transformed_df = get_zfc_compatible_df(df,
                                                   exp_col=exp_col,
                                                   ctrl_col=ctrl_col,
                                                   output=output,
                                                   gene_col=gene_col,
                                                   sgRNA_col=sgRNA_col,
                                                   UMI_col=UMI_col)
            command = "zfc --input {0} -o {1}".format(output, output + '_zfc')
            primary_output = "{}_zfc_gene.txt".format(output)
            secondary_output = None
        os.system(command)
        if secondary_output is None:
            return pd.read_table(primary_output)
        else:
            if test == 'mageck-ibar':
                genecol = 'group_id'
                df_to_be_merged = pd.merge(primary_output,
                                           secondary_output,
                                           on=genecol,
                                           how='outer',
                                           suffixes=('_high', '_low'))
                return df_to_be_merged
            else:
                return pd.read_table(primary_output), pd.read_table(
                    secondary_output)
    elif type(exp_col) is list:
        dfs_to_be_merged = [
            run_alternative_test(df,
                                 test=test,
                                 exp_col=x,
                                 ctrl_col=ctrl_col,
                                 sgRNA_col=sgRNA_col,
                                 gene_col=gene_col,
                                 UMI_col=UMI_col,
                                 output=output) for x in exp_col
        ]
        merged_df = None
        for x, df_to_be_merged in zip(exp_col, dfs_to_be_merged):
            if test == 'mageck-ibar':
                genecol = 'group_id'
                df_to_be_merged = pd.merge(df_to_be_merged[0],
                                           df_to_be_merged[1],
                                           on=genecol,
                                           how='outer',
                                           suffixes=('_high', '_low'))
            else:
                genecol = df_to_be_merged.columns[0]
            df_to_be_merged.columns = [
                col + '_' + x if col != genecol else col
                for col in df_to_be_merged.columns
            ]
            if merged_df is None:
                merged_df = df_to_be_merged
            else:
                merged_df = pd.merge(merged_df, df_to_be_merged, on=genecol)
        return merged_df

    else:
        raise Exception("exp_col must be string, or list of strings")


def count_gpp_output(
    sgRNA_input,
    barcode_input,
    prefix,
    valid_constructs,
    valid_umis,
    conditions,
    output,
    quality_output=None,
    approximate_construct_matching=False,
    min_mean_read_quality_score=0,
    min_min_read_quality_score=0,
    read_quality_start=32,
    read_quality_end=38,
    verbose=True,
):
    """

    Parameters
    ----------
    sgRNA_input :
        
    barcode_input :
        
    prefix :
        
    valid_constructs :
        
    valid_umis :
        
    conditions :
        
    output :
        
    quality_output :
        (Default value = None)
    approximate_construct_matching :
        (Default value = False)
    min_mean_read_quality_score :
        (Default value = 30)
    min_min_read_quality_score :
        (Default value = 0)
    verbose :
        (Default value = True)
    read_quality_start :
        (Default value = 32)
    read_quality_end :
        (Default value = 38)

    Returns
    -------

    
    """
    from tqdm import tqdm
    import pyfastx
    if approximate_construct_matching:
        import Levenshtein

    constructs = np.genfromtxt(valid_constructs, dtype=str)
    umis = np.genfromtxt(valid_umis, dtype=str)
    conditions = pd.read_csv(conditions, header=None).set_index(0)[1].to_dict()

    construct2counts = {
        condition: {
            construct: {
                umi: 0
                for umi in umis
            }
            for construct in constructs
        }
        for condition in conditions.keys()
        if type(conditions[condition]) == str
    }
    if quality_output is not None:
        construct2quality = {
            condition: {
                construct: {
                    umi: 0
                    for umi in umis
                }
                for construct in constructs
            }
            for condition in conditions.keys()
            if type(conditions[condition]) == str
        }

    sgrna = pyfastx.Fastx(sgRNA_input)
    index = pyfastx.Fastx(barcode_input)
    success_counter = 0
    read_qc_filter_counter = 0
    counter = 0
    for line_sgrna, line_index in zip(
            tqdm(sgrna, desc='looping through fastq'), index):
        if len(line_sgrna) == 4:
            name_sgrna, seq_sgrna, qual_sgrna, comment_sgrna = line_sgrna
            name_barcode, seq_barcode, qual_barcode, comment_barcode = line_index
        elif len(line_sgrna) == 3:
            name_sgrna, seq_sgrna, qual_sgrna = line_sgrna
            name_barcode, seq_barcode, qual_barcode = line_index
        else:
            raise Exception("Unexpected format of fastq!")
        counter += 1
        try:
            construct, umi = seq_sgrna.split(prefix)
            umi = umi[0:6]
            umi_quality = np.array([
                ord(x) - 33 for x in qual_sgrna
            ])[read_quality_start:(read_quality_end + 1)]
            if (np.mean(umi_quality) >= min_mean_read_quality_score) and (
                    np.min(umi_quality) >= min_min_read_quality_score):
                construct2counts[seq_barcode][construct][umi] += 1
                if quality_output is not None:
                    construct2quality[seq_barcode][construct][
                        umi] += umi_quality
                success_counter += 1
            else:
                read_qc_filter_counter += 1

        except:
            if approximate_construct_matching:
                try:
                    construct, umi = seq_sgrna.split(prefix)
                    umi = umi[0:6]
                    distance = Levenshtein.hamming
                    distances = [distance(construct, x) for x in constructs]
                    if np.min(distances) == 1:
                        construct = constructs[np.argmin(distances)]
                    construct2counts[seq_barcode][construct][umi] += 1
                    if quality_output is not None:
                        construct2quality[seq_barcode][construct][
                            umi] += np.mean([ord(x) - 33 for x in qual_sgrna])
                except:
                    try:
                        construct, umi = seq_sgrna.split(prefix)
                        umi = umi[0:6]
                    except:
                        pass

    df1 = pd.concat([
        pd.DataFrame.from_dict(construct2counts[key]).stack().rename(
            conditions[key]) for key in construct2counts.keys()
    ],
                    axis=1)
    df1.index.rename(['UMI', 'Construct'], inplace=True)
    df1.to_csv(output, index=True)
    if quality_output is not None:
        df2 = pd.concat([
            pd.DataFrame.from_dict(construct2quality[key]).stack().rename(
                conditions[key]) for key in construct2quality.keys()
        ],
                        axis=1)
        df2.index.rename(['UMI', 'Construct'], inplace=True)
        df2 = df2 / df1
        df2.to_csv(quality_output, index=True)
    if verbose:
        print('{0:.2f}%'.format(100 * success_counter / counter),
              'of reads successfully counted')
        print('{0:.2f}%'.format(100 * read_qc_filter_counter / counter),
              'of reads ignored due to low qc')


def apply_cross_sample_filter_to_grna_umi_counts(counts_matrix,
                                                 subjects,
                                                 threshold_factor=1e-5,
                                                 subject_blacklist=[]):
    """

    Parameters
    ----------
    counts_matrix :
        
    subjects :
        
    threshold_factor :
         (Default value = 1e-5)
    subject_blacklist :
         (Default value = [])

    Returns
    -------

    """
    from tqdm import tqdm
    import numpy as np
    samples = counts_matrix.columns
    counts_matrix_filtered = counts_matrix.copy()
    for sample, subject in zip(tqdm(samples), subjects):
        non_subject_samples = ~np.isin(subjects, [subject] + subject_blacklist)
        counts_matrix_filtered[sample] = counts_matrix_filtered[sample] * (
            counts_matrix_filtered[sample]
            > counts_matrix.iloc[:, non_subject_samples].max(axis=1) *
            threshold_factor)
    return counts_matrix_filtered


def apply_threshold_to_grna_umi_counts(
        counts,
        barcodes=None,
        fixed_threshold=0,
        per_sample_read_fraction=1e-6,
        per_sample_fixed_threshold=None,
        number_of_cells_recovered=None,
        per_sample_per_barcode_of_expected_threshold=1e-3,
        faust_probabilistic_model_error_rate=1e-6,
        per_recovered_cell_read_fraction=1e-3,
        mode='fixed'):
    """

    Parameters
    ----------
    counts :
        
    barcodes :
         (Default value = None)
    fixed_threshold :
         (Default value = 0)
    per_sample_read_fraction :
         (Default value = 1e-6)
    per_sample_fixed_threshold :
         (Default value = None)
    number_of_cells_recovered :
         (Default value = None)
    per_sample_per_barcode_of_expected_threshold :
         (Default value = 1e-3)
    faust_probabilistic_model_error_rate :
         (Default value = 1e-6)
    per_recovered_cell_read_fraction :
         (Default value = 1e-3)
    mode :
         (Default value = 'fixed')

    Returns
    -------

    """
    import numpy as np
    import pandas as pd
    if mode not in [
            'fixed', 'per_sample', 'per_sample_per_barcode',
            'faust_probabilistic_model', 'per_sample_fixed_threshold',
            'per_sample_cell_recovery_adjusted'
    ]:
        raise Exception(
            "mode must be in ['fixed','per_sample','per_sample_per_barcode','faust_probabilistic_model','per_sample_fixed_threshold',per_sample_cell_recovery_adjusted']"
        )
    if mode == 'fixed':
        return counts * (counts > fixed_threshold)
    elif mode == 'per_sample':
        return counts * ((counts / np.sum(counts)) > per_sample_read_fraction)
    elif mode == 'per_sample_fixed_threshold':
        if barcodes is None:
            raise Exception(
                'barcodes must not be None when mode == "per_sample_fixed_threshold"'
            )
        if len(barcodes) != len(counts):
            raise Exception(
                'barcodes must be iterable with length equal to counts')
        return counts * (counts > per_sample_fixed_threshold)
    elif mode == 'per_sample_cell_recovery_adjusted':
        if number_of_cells_recovered is None:
            raise Exception(
                'number_of_cells_recovered must not be None when mode == "per_sample_cell_recovery_adjusted"'
            )
        return counts * (
            (counts / np.sum(counts)
             > per_recovered_cell_read_fraction / number_of_cells_recovered))
    elif mode == 'per_sample_per_barcode':
        if barcodes is None:
            raise Exception(
                'barcodes must not be None when mode == "per_sample_per_barcode"'
            )
        if number_of_cells_recovered is None:
            raise Exception(
                'number_of_cells_recovered must not be None when mode == "per_sample_per_barcode"'
            )
        if len(barcodes) != len(counts):
            raise Exception(
                'barcodes must be iterable with length equal to counts')
        if type(counts) == pd.core.series.Series:
            counts = counts.values
        df = pd.DataFrame(counts, columns=['count'])
        df['barcode'] = barcodes
        barcode2barcode_sum = df.groupby('barcode')['count'].sum().to_dict()
        df['barcode_sum'] = [
            barcode2barcode_sum[x] if barcode2barcode_sum[x] > 0 else np.nan
            for x in barcodes
        ]

        df['normalized_counts'] = df['count'] / df['barcode_sum']
        n_barcodes = len(np.unique(barcodes))
        df['threshold'] = (number_of_cells_recovered / n_barcodes
                           ) * per_sample_per_barcode_of_expected_threshold
        df['filtered_counts'] = df['count'] * (df['normalized_counts']
                                               > df['threshold'])
        return df['filtered_counts'].values
    elif mode == 'faust_probabilistic_model':
        if barcodes is None:
            raise Exception(
                'barcodes must not be None when mode == "faust_probabilistic_model"'
            )
        if len(barcodes) != len(counts):
            raise Exception(
                'barcodes must be iterable with length equal to counts')
        if number_of_cells_recovered is None:
            raise Exception(
                'number_of_cells_recovered must not be None when mode == "faust_probabilistic_model"'
            )
        from scipy.stats import poisson
        import pandas as pd
        import numpy as np
        if type(counts) == pd.core.series.Series:
            counts = counts.values
        df = pd.DataFrame(counts, columns=['count'])
        df['barcode'] = barcodes
        n_barcodes = len(np.unique(barcodes))

        def get_per_barcode_threshold_using_probabilistic_model(
                umi_counts,
                n_sorted_cells,
                total_grna=6000,
                umi_error_rate=1e-6):
            """

            Parameters
            ----------
            umi_counts :
                
            n_sorted_cells :
                
            total_grna :
                 (Default value = 6000)
            umi_error_rate :
                 (Default value = 1e-6)

            Returns
            -------

            """
            from scipy.stats import poisson
            umi_counts = np.sort(umi_counts.values)[::-1]
            if np.sum(umi_counts) == 0:
                return np.nan
            l_true = n_sorted_cells / total_grna
            l_read_error = umi_counts[0] * umi_error_rate
            #            print(l_read_error)
            prob_single_read_error = (1 - poisson.cdf(1, l_read_error))
            #            print(prob_single_read_error)
            if not np.isnan(prob_single_read_error):
                for j in range(1, len(umi_counts) + 1):
                    prob_true = (1 - poisson.cdf(j, l_true)) / (
                        1 - poisson.cdf(1, l_true))
                    if prob_true < prob_single_read_error:
                        break
#                if j < len(umi_counts):
                return umi_counts[j - 1]

#                else:
#                    return 0
            else:
                return 0

        barcode2threshold = df.groupby('barcode')['count'].apply(
            get_per_barcode_threshold_using_probabilistic_model,
            *(number_of_cells_recovered, n_barcodes,
              faust_probabilistic_model_error_rate)).to_dict()
        #print(barcode2threshold)
        df['threshold'] = [barcode2threshold[x] for x in df['barcode'].values]
        #        display(df['threshold'])
        return df['count'].values * (df['count'].values
                                     > df['threshold'].values)




def poisson_gmm_threshold(counts,
                          n_component=2,
                          prethreshold=1,
                          random_state=17):
    from pomegranate.gmm import GeneralMixtureModel
    from pomegranate.distributions import Poisson
    import pandas as pd
    model = GeneralMixtureModel([Poisson() for x in range(n_component)],
                                verbose=False,
                                max_iter=100,
                                tol=0.01,
                                random_state=random_state)
    X = np.log(counts[counts > prethreshold]).values.reshape(-1, 1)

    model.fit(X, )
    predictions = np.array(model.predict(X))

    prediction_df = pd.DataFrame(counts[counts > prethreshold].values,
                                 columns=['counts'])
    prediction_df['prediction'] = predictions
    top_component = prediction_df.groupby('prediction').mean().sort_values(
        'counts', ascending=False).index.values[0]
    return counts[counts > prethreshold][predictions ==
                                         top_component].min() - 1


#from faust.utilities import get_engraftment
#import seaborn as sns
#import matplotlib.pyplot as plt


def get_engraftment(counts,
                    mode,
                    inputnumber,
                    recoverednumber,
                    barcodes,
                    fixed_threshold=1,
                    faust_probabilistic_model_error_rate=1e-6):
    """

    Parameters
    ----------
    counts : List of gRNA-UMI counts
        
    mode : One of 'nothreshold','fixed','per_sample','per_sample_cell_recovery_adjusted','per_sample_per_barcode', 'faust_probabilistic_model','combined'
        
    inputnumber : Number of cells in input aliquot (pre-engraftment cell count)
        
    recoverednumber : Number of cells recovered by output, as counted by flow
        
    barcodes : list of barcode (gRNA) names (identical in length to list of counts)
        

    Returns
    -------

    """
    from faust.utilities import apply_threshold_to_grna_umi_counts
    if mode == 'nothreshold':
        filtered_data = apply_threshold_to_grna_umi_counts(counts,
                                                           fixed_threshold=0,
                                                           mode='fixed')
        anno = 'no filtering/thresholding applied'

    elif mode == 'fixed':
        filtered_data = apply_threshold_to_grna_umi_counts(
            counts, fixed_threshold=fixed_threshold, mode=mode)
        anno = 'fixed threshold (>{} count considered)'.format(fixed_threshold)

    elif mode == 'per_sample':
        filtered_data = apply_threshold_to_grna_umi_counts(counts, mode=mode)
        anno = 'per-sample threshold (reads normalized to (median over all samples)/(sample total), counts>1 considered)'
    elif mode == 'per_sample_cell_recovery_adjusted':
        filtered_data = apply_threshold_to_grna_umi_counts(
            counts,
            number_of_cells_in_sample=recoverednumber,
            mode=mode,
            per_recovered_cell_read_fraction=0.01)
        anno = 'per-sample threshold (reads normalized to (number of cells recovery)/(sample total), counts>{} considered)'.format(
            per_recovered_cell_read_fraction)
    elif mode == 'per_sample_per_barcode':
        filtered_data = apply_threshold_to_grna_umi_counts(
            counts,
            barcodes=barcodes,
            number_of_cells_recovered=recoverednumber,
            mode=mode)
        anno = 'per-sample threshold (reads normalized to (median over all samples)/(sample total), counts>1 considered)'

    elif mode == 'faust_probabilistic_model':
        filtered_data = apply_threshold_to_grna_umi_counts(
            counts,
            barcodes=barcodes,
            number_of_cells_recovered=recoverednumber,
            mode=mode,
            faust_probabilistic_model_error_rate=
            faust_probabilistic_model_error_rate)
        anno = 'FAUST probabilistic model'

    elif mode == 'combined':
        filtered_data1 = apply_threshold_to_grna_umi_counts(
            counts,
            barcodes=barcodes,
            number_of_cells_recovered=recoverednumber,
            mode='per_sample_cell_recovery_adjusted',
        )
        filtered_data2 = apply_threshold_to_grna_umi_counts(
            counts,
            barcodes=barcodes,
            number_of_cells_recovered=recoverednumber,
            mode='faust_probabilistic_model',
            faust_probabilistic_model_error_rate=
            faust_probabilistic_model_error_rate)
        filtered_data = counts * (filtered_data1 > 0) * (filtered_data2 > 0)
        anno = 'combination of FAUST probabilistic model and per-sample normalization (thresholds computed separately, intersection taken thereof)'

    predicted = predicted_input(len(counts), (filtered_data > 0).sum())

    return predicted, predicted / inputnumber, anno


def morisita(counts1, counts2):
    """

    Parameters
    ----------
    counts1 :
        
    counts2 :
        

    Returns
    -------

    
    """
    from faust.utilities import simpson
    simpson1 = simpson(counts1, with_replacement=True)
    simpson2 = simpson(counts2, with_replacement=True)
    cross = (counts1 * counts2).sum() / counts1.sum() / counts2.sum()
    return 2 * cross / (simpson1 + simpson2)


def simpson(x, with_replacement=False):
    """For computing simpson index directly from counts (or frequencies, if with_replacement=True)

    Parameters
    ----------
    x :
        
    with_replacement :
        (Default value = False)

    Returns
    -------

    
    """
    total = np.sum(x)
    if with_replacement:
        return np.sum([(y / total) * (y / total) for y in x])
    else:
        return np.sum([(y / total) * ((y - 1) / (total - 1)) for y in x])


def scale_across_pools(faust_output,
                       pool_col='pool',
                       genes_for_scaling=[],
                       output_type_column='input->output',
                       common_language_effect_col='CommonLanguageEffectSize'):
    if len(genes_for_scaling) == 0:
        raise Exception("must include genes to scale against")
    from scipy.special import logit, expit
    faust_output['logit(CLES)'] = faust_output[
        common_language_effect_col].apply(logit)
    io2pool2factor = {}
    for io in faust_output[output_type_column].unique():
        scalefactors_list = []
        for gene in genes_for_scaling:
            factor = np.median(
                faust_output[faust_output[output_type_column] == io].query(
                    'gene==@gene').groupby(pool_col)['logit(CLES)'].apply(
                        np.nanmedian).values)
            scalefactors = factor / faust_output[
                faust_output[output_type_column] == io].query(
                    'gene==@gene').groupby(pool_col)['logit(CLES)'].apply(
                        np.nanmedian)
            scalefactors_list.append(scalefactors)
        scalefactor_df = pd.concat(scalefactors_list, axis=1)
        #scalefactor
        io2pool2factor[io] = scalefactor_df.apply(np.nanmedian,
                                                  axis=1).to_dict()
    faust_output['scaled_logit(CLES)'] = faust_output.apply(
        lambda x: io2pool2factor[x[output_type_column]][x[pool_col]] * x[
            'logit(CLES)'],
        axis=1)
    faust_output['scaled_CLES'] = faust_output['scaled_logit(CLES)'].apply(
        expit)
    return faust_output['scaled_CLES'].values
